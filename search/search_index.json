{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bem vindo ao meu projeto!","text":"<p>Fun\u00e7\u00e3o Extract </p> <p>\" Web-scraping function to read all pages in airbnb and save the results in a dataframe.</p>"},{"location":"#src.etl.extract.extract--parameters","title":"Parameters","text":"<p>destination : str     Destination of places to scrape on airbnb.</p> bool, default = True <p>If False, the navigator will not be minimized while doing the web-scraping.</p> bool, default = False <p>If True, the dataframe will be exported as a csv file</p> <p>Return: A pandas Dataframe with the webscraping data</p> Source code in <code>src\\etl\\extract.py</code> <pre><code>def extract(destination, headless=True, export=True):\n\n    \"\"\" \"\n    Web-scraping function to read all pages in airbnb and save the results in a dataframe.\n\n    Parameters\n    ----------\n    destination : str\n        Destination of places to scrape on airbnb.\n\n    headless : bool, default = True\n        If False, the navigator will not be minimized while doing the web-scraping.\n\n    export_to_csv : bool, default = False\n        If True, the dataframe will be exported as a csv file\n\n    Return: A pandas Dataframe with the webscraping data\n    \"\"\"\n\n    # creating list named data that will receive all data from airbnb\n    data = []\n\n    # setting options\n    options = Options()\n    options.add_argument('--disable-logging')\n    options.add_argument('--log-level=3')\n\n    if headless == True:\n        options.add_argument('--headless')\n\n    # set url and destination to search\n    main_url = 'https://www.airbnb.com.br/'\n    destination = destination\n\n    # going to places adress\n    navegador = webdriver.Edge(options=options)\n    navegador.get(main_url)\n\n    sleep(2)\n\n    # clicking on where button\n    where_button = navegador.find_element(\n        'xpath', \"//input[@id='bigsearch-query-location-input']\"\n    )\n\n    try:\n        where_button.click()\n    except ElementNotInteractableException:\n        where_button = navegador.find_element(\n            'xpath', \"//button[span='Localiza\u00e7\u00e3o']\"\n        )\n        where_button.click()\n\n    sleep(2)\n\n    # sending keys to search bar\n    search_bar = navegador.find_element(\n        'xpath', \"//input[@autocomplete='off']\"\n    )\n    search_bar.send_keys(destination)\n\n    sleep(1)\n\n    # click search button\n    search_button = navegador.find_element(\n        'xpath',\n        \"//button[@data-testid='structured-search-input-search-button']\",\n    )\n    search_button.click()\n\n    sleep(2)\n    page_content = navegador.page_source\n    site = BeautifulSoup(page_content, 'html.parser')\n\n    # finding the list item element\n    places = site.findAll('div', attrs={'itemprop': 'itemListElement'})\n    pages = site.find(\n        'nav', attrs={'aria-label': 'Pagina\u00e7\u00e3o de resultados de busca'}\n    )\n    n_pages = int(pages.findAll('a')[-2].text)\n\n    # clicking the accept cookies button, so we are able to click on next page\n\n    cookies = navegador.find_element(\n        'xpath', \"//div[@data-testid='main-cookies-banner-container']\"\n    )\n    cookies_button = cookies.find_elements('xpath', './/button')[1]\n    cookies_button.click()\n\n    # scrolling all the way down to load the entire page\n    for page in range(n_pages - 1):\n        sleep(2)\n        navegador.execute_script('window.scrollTo(0, 1000)')\n        sleep(0.5)\n\n        navegador.execute_script('window.scrollTo(0, 2000)')\n        sleep(0.5)\n\n        navegador.execute_script('window.scrollTo(0, 3000)')\n        sleep(1)\n\n        # getting the page content\n        page_content = navegador.page_source\n        site = BeautifulSoup(page_content, 'html.parser')\n        places = site.findAll('div', attrs={'itemprop': 'itemListElement'})\n\n        # scrape all the places for each page\n        for place in places:\n            # title\n            place_title = place.find(\n                'div', attrs={'data-testid': 'listing-card-title'}\n            ).text\n\n            # subtitle\n            place_desc = place.find('meta', attrs={'itemprop': 'name'})[\n                'content'\n            ]\n\n            # url\n            place_url = place.find('meta', attrs={'itemprop': 'url'})[\n                'content'\n            ]\n\n            # beds description\n            beds = place.findAll(\n                'div', attrs={'data-testid': 'listing-card-subtitle'}\n            )[1].text\n\n            # price\n            price_div = place.findAll('div', attrs={'aria-hidden': 'true'})[-1]\n\n            place_price = (\n                price_div.findAll('span')[1]\n                .get_text(strip=True)\n                .replace('R$', '')\n                .replace('noite', '')\n                .replace('.', '')\n            )\n\n            try:\n                place_price = float(place_price)\n\n            except:\n                place_price = (\n                    price_div.findAll('span')[0]\n                    .get_text(strip=True)\n                    .replace('R$', '')\n                    .replace('noite', '')\n                    .replace('.', '')\n                )\n\n                place_price = float(place_price)\n\n            # if the place is superhost\n            superhost = place.find(\n                'div', attrs={'aria-describedby': 'carousel-description'}\n            )\n            place_host = superhost.findAll('div')[4].text\n            if place_host == 'Superhost':\n                place_host = 'Yes'\n            else:\n                place_host = 'No'\n\n            data.append(\n                [\n                    place_title,\n                    place_desc,\n                    place_url,\n                    place_price,\n                    beds,\n                    place_host,\n                ]\n            )\n\n        next_button = navegador.find_element(\n            'xpath', \"//a[@aria-label='Pr\u00f3ximo']\"\n        )\n        next_button.click()\n\n    # creating pandas dataframe\n    columns_names = [\n        'title',\n        'subtitle',\n        'url',\n        'price_per_night',\n        'beds',\n        'is_superhost',\n    ]\n    df = pd.DataFrame(data, columns=columns_names)\n\n    # exporting to csv\n    if export == True:\n        current_directory = os.path.dirname(os.path.realpath(__file__))\n        output_dir = os.path.join(current_directory, 'output')\n        output_file = f'{destination}.csv'\n\n        output_path = os.path.join(output_dir, output_file)\n        df.to_csv(output_path, index=False, sep=';')\n    return df\n</code></pre>"}]}